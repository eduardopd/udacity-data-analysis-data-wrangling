{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wrangling Report\n",
    "\n",
    "#### Gather\n",
    "The wrangling efforts start by gathering the data. There were three sources of information, a CSV, provided to us, a TSV that I needed to get using the requests library, and the Twitter API. The first two were quite simple to gather but building the request to the Twitter API was a bit tricky. I spent some time figuring out how to code the request and found out that defining the encoding was a must. Another important step requesting the information from the API was adding a \"\\n\" at the end of each JSON written line. At the end of the gathering process, I had three data frames populated with images, archives, and API data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess and Clean\n",
    "\n",
    "Assessing and cleaning processes could begin. The archive's data frame was by far the most complicated one. Firstly, I started by doing a visual assessment and found some strange numerators, they were too large, and four weird-looking columns labeled as \"doggo, floofer, pupper, and puppo\". I knew those were dog stages by The Dogtionary, but there were not displayed properly. By visually assessing, I also verified that the source column contained an HTML Tag relative to a link. After the Visual Assessment, I started to programmatically assess the data using both info() and describe() methods. These methods showed me some issues regarding the expanded_urls column, some values were missing. \n",
    "\n",
    "The info() method provided me with information about the columns data types and, some dates were formatted as strings. After listing all the issues regarding quality and tidiness, I defined them as tasks. Cleaning the data frames was very challenging and I had to do a lot of research, especially in Stack Overflow. I had to use regular expressions to format the dates and extract the tags from the source column. I also used the replace and capitalize methods to clean the image's data frame. After cleaning quality issues I started cleaning tidiness issues. Removing unused columns and creating a new column only for the dog stages defined by The Dogtionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and Export\n",
    "\n",
    "Finally, since we only wanted tweets with images, I started to merge the data frames into one large and major data frame using the id as the primary key. The result was exported to a CSV file, cleaned and assessed! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
